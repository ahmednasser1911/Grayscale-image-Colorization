{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input/model1/mixed1/Mixed1.hdf5\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.python.client import device_lib\nfrom keras import backend as K\n\nimport os, sys, threading\n\nimport numpy as np\nimport tensorflow as tf\n\nimport keras\nfrom keras.utils import multi_gpu_model\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, TensorBoard\nfrom keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\nfrom keras.applications.inception_resnet_v2 import InceptionResNetV2, preprocess_input\nfrom keras.layers.core import RepeatVector, Permute\nfrom keras.models import Model\nfrom keras.layers import Conv2D, UpSampling2D, InputLayer, Conv2DTranspose, Input, Reshape, merge, concatenate\nfrom keras.initializers import TruncatedNormal\nfrom keras.optimizers import RMSprop\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils import shuffle\n\nfrom skimage.color import rgb2lab, lab2rgb, rgb2gray, gray2rgb\nfrom skimage.transform import resize\nfrom skimage.io import imsave\n\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef batch_apply(ndarray, func, *args, **kwargs):\n    \"\"\"Calls func with samples, func should take ndarray as first positional argument\"\"\"\n\n    batch = []\n    for sample in ndarray:\n        batch.append(func(sample, *args, **kwargs))\n    return np.array(batch)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"inception = InceptionResNetV2(weights='imagenet', include_top=True)\ninception.graph = tf.get_default_graph()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef create_inception_embedding(grayscaled_rgb):\n    '''Takes (299, 299, 3) RGB and returns the embeddings(predicions) generated on the RGB image'''\n    with inception.graph.as_default():\n        embed = inception.predict(grayscaled_rgb)\n    return embed","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_model():\n    embed_input = Input(shape=(1000,))\n    encoder_input = Input(shape=(256, 256, 1,))\n\n    #Encoder\n    encoder_output = Conv2D(64, (3,3), activation='relu', padding='same', strides=2,\n                            bias_initializer=TruncatedNormal(mean=0.0, stddev=0.05))(encoder_input)\n    encoder_output = Conv2D(128, (3,3), activation='relu', padding='same',\n                            bias_initializer=TruncatedNormal(mean=0.0, stddev=0.05))(encoder_output)\n    encoder_output = Conv2D(128, (3,3), activation='relu', padding='same', strides=2,\n                            bias_initializer=TruncatedNormal(mean=0.0, stddev=0.05))(encoder_output)\n    encoder_output = Conv2D(256, (3,3), activation='relu', padding='same',\n                            bias_initializer=TruncatedNormal(mean=0.0, stddev=0.05))(encoder_output)\n    encoder_output = Conv2D(256, (3,3), activation='relu', padding='same', strides=2,\n                            bias_initializer=TruncatedNormal(mean=0.0, stddev=0.05))(encoder_output)\n    encoder_output = Conv2D(512, (3,3), activation='relu', padding='same',\n                            bias_initializer=TruncatedNormal(mean=0.0, stddev=0.05))(encoder_output)\n    encoder_output = Conv2D(512, (3,3), activation='relu', padding='same',\n                            bias_initializer=TruncatedNormal(mean=0.0, stddev=0.05))(encoder_output)\n    encoder_output = Conv2D(256, (3,3), activation='relu', padding='same',\n                            bias_initializer=TruncatedNormal(mean=0.0, stddev=0.05))(encoder_output)\n\n    #Fusion\n    fusion_output = RepeatVector(32 * 32)(embed_input)\n    fusion_output = Reshape(([32, 32, 1000]))(fusion_output)\n    fusion_output = concatenate([encoder_output, fusion_output], axis=3)\n    fusion_output = Conv2D(256, (1, 1), activation='relu', padding='same',\n                            bias_initializer=TruncatedNormal(mean=0.0, stddev=0.05))(fusion_output)\n\n    #Decoder\n    decoder_output = Conv2D(128, (3,3), activation='relu', padding='same',\n                            bias_initializer=TruncatedNormal(mean=0.0, stddev=0.05))(fusion_output)\n    decoder_output = UpSampling2D((2, 2))(decoder_output)\n    decoder_output = Conv2D(64, (3,3), activation='relu', padding='same',\n                            bias_initializer=TruncatedNormal(mean=0.0, stddev=0.05))(decoder_output)\n    decoder_output = UpSampling2D((2, 2))(decoder_output)\n    decoder_output = Conv2D(32, (3,3), activation='relu', padding='same',\n                            bias_initializer=TruncatedNormal(mean=0.0, stddev=0.05))(decoder_output)\n    decoder_output = Conv2D(16, (3,3), activation='relu', padding='same',\n                            bias_initializer=TruncatedNormal(mean=0.0, stddev=0.05))(decoder_output)\n    decoder_output = Conv2D(2, (3, 3), activation='tanh', padding='same',\n                            bias_initializer=TruncatedNormal(mean=0.0, stddev=0.05))(decoder_output)\n    decoder_output = UpSampling2D((2, 2))(decoder_output)\n\n    model = Model(inputs=[encoder_input, embed_input], outputs=decoder_output)\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"datagen = ImageDataGenerator(shear_range=0.2, zoom_range=0.2, rotation_range=20, horizontal_flip=True)\n\n# Convert images to LAB format and resizes to 256 x 256 for Encoder input.\n# Also, generates Inception-resnet embeddings and returns the processed batch\n\ndef process_images(rgb, input_size=(256, 256, 3), embed_size=(299, 299, 3)):\n    \"\"\"Takes RGB images in float representation and returns processed batch\"\"\"\n\n    # Resize for embed and Convert to grayscale\n    gray = gray2rgb(rgb2gray(rgb))\n    gray = batch_apply(gray, resize, embed_size, mode='constant')\n    # Zero-Center [-1, 1]\n    gray = gray * 2 - 1\n    # Generate embeddings\n    embed = create_inception_embedding(gray)\n\n    # Resize to input size of model\n    re_batch = batch_apply(rgb, resize, input_size, mode='constant')\n    # RGB => L*a*b*\n    re_batch = batch_apply(re_batch, rgb2lab)\n\n    # Extract L* into X, zero-center and normalize\n    X_batch = re_batch[:,:,:,0]\n    X_batch = X_batch/50 - 1\n    X_batch = X_batch.reshape(X_batch.shape+(1,))\n\n    # Extract a*b* into Y and normalize. Already zero-centered.\n    Y_batch = re_batch[:,:,:,1:]\n    Y_batch = Y_batch/128\n\n    return [X_batch, embed], Y_batch\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def image_a_b_gen(images, batch_size):\n    while True:\n        for batch in datagen.flow(images, batch_size=batch_size):\n            print(\"batch proccessed\")\n            yield process_images(batch)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"DATASET = \"../input/cliffabbeynature2/cliffabbeynature2/CliffAbbeyNature2/\"\nfrom tqdm import tqdm\n# Get images file names\ntraining_files, testing_files = train_test_split(shuffle(os.listdir(DATASET)), test_size=0.03)\nprint(len(training_files))\ndef getImages(DATASET, filelist, transform_size=(299, 299, 3)):\n    \"\"\"Reads JPEG filelist from DATASET and returns float represtation of RGB [0.0, 1.0]\"\"\"\n    img_list = []\n    i=0\n    for filename in tqdm(filelist):\n        # Loads JPEG image and converts it to numpy float array.\n        image_in = img_to_array(load_img(DATASET + filename))\n\n        # [0.0, 255.0] => [0.0, 1.0]\n        image_in = image_in/255\n        #print(i)\n        i+=1\n        if transform_size is not None:\n            image_in = resize(image_in, transform_size, mode='reflect')\n\n        img_list.append(image_in)\n    img_list = np.array(img_list)\n\n    return img_list","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = build_model()\n#model.load_weights('../input/model1/mixed1/')\n\nmodel.compile(optimizer=RMSprop(lr=1e-3), loss='mse',metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef train(model, training_files, batch_size=8, epochs=500, steps_per_epoch=100):\n    print('Trains the model')\n    training_set = getImages(DATASET, training_files)\n    train_size = int(len(training_set)*0.90)\n    \n    train_images = training_set[:train_size]\n    val_images = training_set[train_size:]\n    val_steps = (len(val_images)//batch_size)\n    print(\"Training samples:\", train_size, \"Validation samples:\", len(val_images))\n\n    callbacks = [\n        EarlyStopping(monitor='val_loss', patience=15, verbose=1, min_delta=1e-5),\n        ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, cooldown=0, verbose=1, min_lr=1e-8),\n        ModelCheckpoint(monitor='val_acc', filepath='Mixed13.hdf5', verbose=1,\n                         save_best_only=True, save_weights_only=True, mode='auto')\n    ]\n\n    history = model.fit_generator(image_a_b_gen(train_images, batch_size), epochs=epochs,\n                                 steps_per_epoch=steps_per_epoch,\n                        verbose=1, callbacks=callbacks, validation_data=process_images(val_images))\n    \n    return history","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = train(model, training_files, epochs=100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nimport csv\n\ndef writeToCsv(acc_lst, loss_lst, val_acc_lst, val_loss_lst):    \n   \n    filename = 'training_metrics.csv'\n\n    isOld = os.path.exists(os.path.join(filename))   \n    \n    with open (filename, 'a', newline='') as fp:\n        a = csv.writer(fp)      \n        if isOld == False:\n            row = ['Accuracy', 'Loss', 'Val_Accuracy', 'Val_Loss']\n            a.writerow(row)\n        for acc, loss, val_acc, val_loss in zip(acc_lst, loss_lst, val_acc_lst, val_loss_lst):              \n            row = [str(acc), str(loss), str(val_acc), str(val_loss)]\n            a.writerow(row)\n            \n            \ndef plot_training_history(history):\n    # Get the classification accuracy and loss-value\n    # for the training-set.\n    acc = history.history['acc']\n    loss = history.history['loss']\n    \n    # Get it for the validation-set (we only use the test-set).\n    val_acc = history.history['val_acc']\n    val_loss = history.history['val_loss']\n\n    writeToCsv(acc, loss, val_acc, val_loss)\n    \n    # Plot the accuracy and loss-values for the training-set.\n    plt.plot(acc, linestyle='-', color='b', label='Training Acc.')\n    plt.plot(loss, 'o', color='b', label='Training Loss')\n    \n    # Plot it for the test-set.\n    plt.plot(val_acc, linestyle='--', color='r', label='Test Acc.')\n    plt.plot(val_loss, 'o', color='r', label='Test Loss')\n\n    # Plot title and legend.\n    plt.title('Training and Test Accuracy')\n    plt.legend()\n\n    # Ensure the plot shows correctly.\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_training_history(history)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def test(model, training_files, save_actual=False, save_gray=False):\n    test_images = getImages(DATASET, training_files)\n    \n    act = getImages(DATASET, training_files)\n    #act = act*255\n    model.load_weights(filepath=\"../input/model1/mixed1/Mixed1.hdf5\")\n\n    print('Preprocessing Images')\n    X_test, Y_test = process_images(test_images)\n\n    print('Predicting')\n    # Test model\n    output = model.predict(X_test)\n\n    # Rescale a*b* back. [-1.0, 1.0] => [-128.0, 128.0]\n    output = output * 128\n    Y_test = Y_test * 128\n    pred = []\n\n    # Output colorizations\n    for i in tqdm(range(len(output))):\n        #name = testing_files[i].split(\".\")[0]\n        #print('Saving '+str(i)+\"th image \" + name + \"_*.png\")\n\n        lightness = X_test[0][i][:,:,0]\n\n        #Rescale L* back. [-1.0, 1.0] => [0.0, 100.0]\n        lightness = (lightness + 1) * 50\n\n        predicted = np.zeros((256, 256, 3))\n        predicted[:,:,0] = lightness\n        predicted[:,:,1:] = output[i]\n        pred.append(predicted)\n        \n    return pred , act\n      ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pr, a= test(model, testing_files)\n\nimport cv2\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(4, 3, figsize=(12,12))\n\npr[78]=lab2rgb(pr[78])\ngray = gray2rgb(rgb2gray(a[78]))\nax[0,0].imshow(gray)\nax[0,1].imshow(pr[78])\nax[0,2].imshow(a[78])\n\npr[79]=lab2rgb(pr[79])\ngray = gray2rgb(rgb2gray(a[79]))\nax[1,0].imshow(gray)\nax[1,1].imshow(pr[79])\nax[1,2].imshow(a[79])\n\n\npr[80]=lab2rgb(pr[80])\ngray = gray2rgb(rgb2gray(a[80]))\nax[2,0].imshow(gray)\nax[2,1].imshow(pr[80])\nax[2,2].imshow(a[80])\n\npr[81]=lab2rgb(pr[81])\ngray = gray2rgb(rgb2gray(a[81]))\nax[3,0].imshow(gray)\nax[3,1].imshow(pr[81])\nax[3,2].imshow(a[81])\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tqdm import tqdm\nimport cv2 \nimport numpy as np\nnames = os.listdir('../input/testingh/test/Test/')\nGS_images = []\nfor i in tqdm(names):\n    path = os.path.join('../input/testingh/test/Test/',i)\n    im = cv2.imread(path)\n    im = im/225\n    GS_images.append(im)\n    \nGS_images = np.array(GS_images)\n\ndef Testing(GS_images,embed_size = (299,299,3) , input_size = (256 ,256 ,3)):\n\n    model.load_weights(filepath=\"../input/model1/mixed1/Mixed1.hdf5\")\n    \n    gray1 = batch_apply(GS_images, resize, embed_size, mode='constant')#299\n    gray1 = gray1 * 2 - 1\n    embed = create_inception_embedding(gray1)\n    \n    gray2 = batch_apply(GS_images, resize, input_size, mode='constant')#256\n    X_batch = gray2[:,:,:,0]\n    X_batch/50-1\n    \n    X_batch = X_batch.reshape(X_batch.shape+(1,))\n    \n    X_test = [X_batch , embed]\n    output = model.predict(X_test)\n\n    # Rescale a*b* back. [-1.0, 1.0] => [-128.0, 128.0]\n    output = output * 128\n    pred = []\n    for i in tqdm(range(len(output))):\n\n        lightness = X_test[0][i][:,:,0]\n\n        #Rescale L* back. [-1.0, 1.0] => [0.0, 100.0]\n        lightness = (lightness + 1) * 50\n\n        predicted = np.zeros((256, 256, 3))\n        predicted[:,:,0] = lightness\n        predicted[:,:,1:] = output[i]\n        pred.append(predicted)\n    return pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"res = Testing(GS_images)\nprint(len(res))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(4, 2, figsize=(38,38))\n\nres[115]=lab2rgb(res[115])\ngray = gray2rgb(rgb2gray(res[115]))\nax[0,0].imshow(gray)\nax[0,1].imshow(res[115])\n\n\nres[116]=lab2rgb(res[116])\ngray = gray2rgb(rgb2gray(res[116]))\nax[1,0].imshow(gray)\nax[1,1].imshow(res[116])\n\nres[55]=lab2rgb(res[55])\ngray = gray2rgb(rgb2gray(res[55]))\nax[2,0].imshow(gray)\nax[2,1].imshow(res[55])\n\nres[65]=lab2rgb(res[65])\ngray = gray2rgb(rgb2gray(res[65]))\nax[3,0].imshow(gray)\nax[3,1].imshow(res[65])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}